{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SimulatedAnnealing",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VexcLFoscA_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.optim import Optimizer\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import math\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZUGvwKDszuK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load IRIS dataset\n",
        "dataset = pd.read_csv('iris.csv')\n",
        "\n",
        "\n",
        "# transform species to numerics\n",
        "dataset.loc[dataset.species=='Iris-setosa', 'species'] = 0\n",
        "dataset.loc[dataset.species=='Iris-versicolor', 'species'] = 1\n",
        "dataset.loc[dataset.species=='Iris-virginica', 'species'] = 2\n",
        "\n",
        "train_X, test_X, train_y, test_y = train_test_split(dataset[dataset.columns[0:4]].values,\n",
        "                                                    dataset.species.values, test_size=0.8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AP-in4clxxoC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    # define nn\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(4, 8)\n",
        "        self.fc2 = nn.Linear(8, 8)\n",
        "        self.fc3 = nn.Linear(8, 3)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, X):\n",
        "        X = F.relu(self.fc1(X))\n",
        "        X = self.fc2(X)\n",
        "        X = self.fc3(X)\n",
        "        X = self.softmax(X)\n",
        "\n",
        "        return X\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1y_uTsR3VjL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SimulatedAnnealing(Optimizer):\n",
        "    def __init__(self, params, startTemp = 1\n",
        "                 , coolRate = 0.001, neighborhoodSize = 5\n",
        "                 , loss = nn.CrossEntropyLoss()\n",
        "                 , model = None\n",
        "                 , features = None\n",
        "                 , labels = None): #these represent default values, but can be overridden\n",
        "        self.startTemp = startTemp\n",
        "        self.coolRate = coolRate\n",
        "        self.currTemp = startTemp\n",
        "        self.loss = loss\n",
        "        self.model = model\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "        self.neighborhoodSize = neighborhoodSize\n",
        "        self.param_groups = []\n",
        "        self.defaults = dict(startTemp=startTemp, coolRate=coolRate, currTemp=startTemp,\n",
        "                        loss=loss,model=model, features = features, labels = labels,\n",
        "                        neighborhoodSize = neighborhoodSize)\n",
        "\n",
        "        param_groups = list(params)\n",
        "        if len(param_groups) == 0:\n",
        "            raise ValueError(\"optimizer got an empty parameter list\")\n",
        "        if not isinstance(param_groups[0], dict):\n",
        "            param_groups = [{'params': param_groups}]\n",
        "\n",
        "        for param_group in param_groups:\n",
        "            self.add_param_group(param_group)\n",
        "\n",
        "    def step(self):\n",
        "        #need to first generate a random new point in the space\n",
        "        oldOutputs = self.model(self.features)\n",
        "        oldLoss = self.loss(oldOutputs, self.labels.type(torch.LongTensor))\n",
        "\n",
        "        # Save init values\n",
        "        old_state_dict = {}\n",
        "        for key in self.model.state_dict():\n",
        "            old_state_dict[key] = self.model.state_dict()[key].clone()\n",
        "\n",
        "        for name, param in self.model.state_dict().items():\n",
        "            # generate a matrix of random changes in each param element to be added to each param matrix\n",
        "            # random = torch.Tensor(np.random.uniform(low = self.neighborhoodSize * -1, high = self.neighborhoodSize\n",
        "                                                        # , size = param.shape))\n",
        "            random = torch.Tensor(np.random.uniform(low = self.neighborhoodSize * -1, high = self.neighborhoodSize\n",
        "                                                        , size = param.shape))\n",
        "            \n",
        "            #now add random to the params to transform them\n",
        "            new_param = param + random\n",
        "            self.model.state_dict()[name].copy_(new_param)\n",
        "\n",
        "        \n",
        "        newOutputs = self.model(self.features)\n",
        "        newLoss = self.loss(newOutputs, self.labels.type(torch.LongTensor))\n",
        "\n",
        "        if (newLoss > oldLoss):\n",
        "            alpha = math.exp(-(newLoss - oldLoss) / self.currTemp)\n",
        "            #print(newLoss.item(), oldLoss.item(), self.currTemp, alpha, -(newLoss - oldLoss) / self.currTemp)\n",
        "            if (np.random.uniform(0, 1) < alpha): \n",
        "              self.model.load_state_dict(old_state_dict)\n",
        "        \n",
        "        self.currTemp -= self.coolRate\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8QvNTe81xO3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# wrap up with Variable in pytorch\n",
        "train_X = Variable(torch.Tensor(train_X).float())\n",
        "test_X = Variable(torch.Tensor(test_X).float())\n",
        "train_y = Variable(torch.Tensor(train_y).long())\n",
        "test_y = Variable(torch.Tensor(test_y).long())\n",
        "\n",
        "\n",
        "net = Net()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()# cross entropy loss\n",
        "\n",
        "\n",
        "#optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
        "optimizer = SimulatedAnnealing(net.parameters(), features=train_X, model=net, labels=train_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laMUElOU2vBf",
        "colab_type": "code",
        "outputId": "23139a24-63eb-46fc-90b7-7ea95a4484db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for epoch in range(10000):\n",
        "    # optimizer.zero_grad()\n",
        "    out = net(train_X)\n",
        "    loss = criterion(out, train_y)\n",
        "    # loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print ('number of epoch', epoch, 'loss', loss.item())\n",
        "\n",
        "predict_out = net(test_X)\n",
        "_, predict_y = torch.max(predict_out, 1)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of epoch 0 loss 1.0847781896591187\n",
            "number of epoch 100 loss 1.0181115865707397\n",
            "number of epoch 200 loss 1.0181115865707397\n",
            "number of epoch 300 loss 1.0181115865707397\n",
            "number of epoch 400 loss 1.0181115865707397\n",
            "number of epoch 500 loss 1.0181115865707397\n",
            "number of epoch 600 loss 1.0181115865707397\n",
            "number of epoch 700 loss 1.0181115865707397\n",
            "number of epoch 800 loss 1.0181115865707397\n",
            "number of epoch 900 loss 1.0181115865707397\n",
            "number of epoch 1000 loss 0.6847781538963318\n",
            "number of epoch 1100 loss 0.6181114912033081\n",
            "number of epoch 1200 loss 0.5514448285102844\n",
            "number of epoch 1300 loss 0.5514448285102844\n",
            "number of epoch 1400 loss 0.5514448285102844\n",
            "number of epoch 1500 loss 0.5514448285102844\n",
            "number of epoch 1600 loss 0.5514448285102844\n",
            "number of epoch 1700 loss 0.5514448285102844\n",
            "number of epoch 1800 loss 0.5514448285102844\n",
            "number of epoch 1900 loss 0.5514448285102844\n",
            "number of epoch 2000 loss 0.5514448285102844\n",
            "number of epoch 2100 loss 0.5514448285102844\n",
            "number of epoch 2200 loss 0.5514448285102844\n",
            "number of epoch 2300 loss 0.5514448285102844\n",
            "number of epoch 2400 loss 0.5514448285102844\n",
            "number of epoch 2500 loss 0.5514448285102844\n",
            "number of epoch 2600 loss 0.5514448285102844\n",
            "number of epoch 2700 loss 0.5514448285102844\n",
            "number of epoch 2800 loss 0.5514448285102844\n",
            "number of epoch 2900 loss 0.5514448285102844\n",
            "number of epoch 3000 loss 0.5514448285102844\n",
            "number of epoch 3100 loss 0.5514448285102844\n",
            "number of epoch 3200 loss 0.5514448285102844\n",
            "number of epoch 3300 loss 0.5514448285102844\n",
            "number of epoch 3400 loss 0.5514448285102844\n",
            "number of epoch 3500 loss 0.5514448285102844\n",
            "number of epoch 3600 loss 0.5514448285102844\n",
            "number of epoch 3700 loss 0.5514448285102844\n",
            "number of epoch 3800 loss 0.5514448285102844\n",
            "number of epoch 3900 loss 0.5514448285102844\n",
            "number of epoch 4000 loss 0.5514448285102844\n",
            "number of epoch 4100 loss 0.5514448285102844\n",
            "number of epoch 4200 loss 0.5514448285102844\n",
            "number of epoch 4300 loss 0.5514448285102844\n",
            "number of epoch 4400 loss 0.5514448285102844\n",
            "number of epoch 4500 loss 0.5514448285102844\n",
            "number of epoch 4600 loss 0.5514448285102844\n",
            "number of epoch 4700 loss 0.5514448285102844\n",
            "number of epoch 4800 loss 0.5514448285102844\n",
            "number of epoch 4900 loss 0.5514448285102844\n",
            "number of epoch 5000 loss 0.5514448285102844\n",
            "number of epoch 5100 loss 0.5514448285102844\n",
            "number of epoch 5200 loss 0.5514448285102844\n",
            "number of epoch 5300 loss 0.5514448285102844\n",
            "number of epoch 5400 loss 0.5514448285102844\n",
            "number of epoch 5500 loss 0.5514448285102844\n",
            "number of epoch 5600 loss 0.5514448285102844\n",
            "number of epoch 5700 loss 0.5514448285102844\n",
            "number of epoch 5800 loss 0.5514448285102844\n",
            "number of epoch 5900 loss 0.5514448285102844\n",
            "number of epoch 6000 loss 0.5514448285102844\n",
            "number of epoch 6100 loss 0.5514448285102844\n",
            "number of epoch 6200 loss 0.5514448285102844\n",
            "number of epoch 6300 loss 0.5514448285102844\n",
            "number of epoch 6400 loss 0.5514448285102844\n",
            "number of epoch 6500 loss 0.5514448285102844\n",
            "number of epoch 6600 loss 0.5514448285102844\n",
            "number of epoch 6700 loss 0.5514448285102844\n",
            "number of epoch 6800 loss 0.5514448285102844\n",
            "number of epoch 6900 loss 0.5514448285102844\n",
            "number of epoch 7000 loss 0.5514448285102844\n",
            "number of epoch 7100 loss 0.5514448285102844\n",
            "number of epoch 7200 loss 0.5514448285102844\n",
            "number of epoch 7300 loss 0.5514448285102844\n",
            "number of epoch 7400 loss 0.5514448285102844\n",
            "number of epoch 7500 loss 0.5514448285102844\n",
            "number of epoch 7600 loss 0.5514448285102844\n",
            "number of epoch 7700 loss 0.5514448285102844\n",
            "number of epoch 7800 loss 0.5514448285102844\n",
            "number of epoch 7900 loss 0.5514448285102844\n",
            "number of epoch 8000 loss 0.5514448285102844\n",
            "number of epoch 8100 loss 0.5514448285102844\n",
            "number of epoch 8200 loss 0.5514448285102844\n",
            "number of epoch 8300 loss 0.5514448285102844\n",
            "number of epoch 8400 loss 0.5514448285102844\n",
            "number of epoch 8500 loss 0.5514448285102844\n",
            "number of epoch 8600 loss 0.5514448285102844\n",
            "number of epoch 8700 loss 0.5514448285102844\n",
            "number of epoch 8800 loss 0.5514448285102844\n",
            "number of epoch 8900 loss 0.5514448285102844\n",
            "number of epoch 9000 loss 0.5514448285102844\n",
            "number of epoch 9100 loss 0.5514448285102844\n",
            "number of epoch 9200 loss 0.5514448285102844\n",
            "number of epoch 9300 loss 0.5514448285102844\n",
            "number of epoch 9400 loss 0.5514448285102844\n",
            "number of epoch 9500 loss 0.5514448285102844\n",
            "number of epoch 9600 loss 0.5514448285102844\n",
            "number of epoch 9700 loss 0.5514448285102844\n",
            "number of epoch 9800 loss 0.5514448285102844\n",
            "number of epoch 9900 loss 0.5514448285102844\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5ZEywma3Blj",
        "colab_type": "code",
        "outputId": "1d377712-315e-4587-93aa-feabc31dfd6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "print ('prediction accuracy', accuracy_score(test_y.data, predict_y.data))\n",
        "\n",
        "print ('macro precision', precision_score(test_y.data, predict_y.data, average='macro'))\n",
        "print ('micro precision', precision_score(test_y.data, predict_y.data, average='micro'))\n",
        "print ('macro recall', recall_score(test_y.data, predict_y.data, average='macro'))\n",
        "print ('micro recall', recall_score(test_y.data, predict_y.data, average='micro'))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "prediction accuracy 0.8583333333333333\n",
            "macro precision 0.873469387755102\n",
            "micro precision 0.8583333333333333\n",
            "macro recall 0.8681818181818182\n",
            "micro recall 0.8583333333333333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFy-vfDTk6p8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}